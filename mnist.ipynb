{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=1024, input_dim=100))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(128*7*7))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Reshape((7, 7, 128), input_shape=(128*7*7,)))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(64, (5, 5), padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(1, (5, 5), padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "            Conv2D(64, (5, 5),\n",
    "            padding='same',\n",
    "            input_shape=(28, 28, 1))\n",
    "            )\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, (5, 5)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[1:3]\n",
    "    image = np.zeros((height*shape[0], width*shape[1]), dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = img[:, :, 0]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(BATCH_SIZE):\n",
    "    # データ読み込み(MNIST)\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "    X_train = np.reshape(X_train, (60000, 28, 28, 1))\n",
    "\n",
    "    # モデル - Adam & CrossEntropy\n",
    "    d = discriminator_model()\n",
    "    d_opt = Adam(lr=2e-4, beta_1=0.5) # 論文通り?_param\n",
    "    #d_opt = Adam(lr=1e-5, beta_1=0.1) # はじめてのGAN_param\n",
    "    d.compile(loss='binary_crossentropy', optimizer=d_opt)\n",
    "    d.trainable = False\n",
    "    g = generator_model()\n",
    "    dcgan = Sequential([g, d])\n",
    "    g_opt = Adam(lr=2e-4, beta_1=0.5)\n",
    "    dcgan.compile(loss='binary_crossentropy', optimizer=g_opt)\n",
    "\n",
    "    # 訓練を進めていく\n",
    "    for epoch in range(100):\n",
    "        # Prints...\n",
    "        print(\"Epoch is\", epoch)\n",
    "        print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n",
    "        # 訓練\n",
    "        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
    "            # noise:(BATCH_SIZE, 100) | Gの入力、-1~1のノイズ\n",
    "            noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100))\n",
    "            # image_batch:(BATCH_SIZE, 28, 28, 1)\n",
    "            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "            # generated_images:(BATCH_SIZE, 28, 28, 1)\n",
    "            generated_images = g.predict(noise, verbose=0)\n",
    "            # 画像を生成して保存\n",
    "            if index % 100 == 0:\n",
    "                image = combine_images(generated_images)\n",
    "                image = image*127.5+127.5\n",
    "                Image.fromarray(image.astype(np.uint8)).save(\"./img/\"+str(epoch)+\"_\"+str(index)+\".png\")\n",
    "            # Dを更新(元画像と生成画像を渡す)\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n",
    "            d_loss = d.train_on_batch(X, y)\n",
    "            print(\"batch %d d_loss : %f\" % (index, d_loss))\n",
    "            # Gを更新\n",
    "            noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "            g_loss = dcgan.train_on_batch(noise, [1] * BATCH_SIZE)\n",
    "            print(\"batch %d g_loss : %f\" % (index, g_loss))\n",
    "            # 重みを定期的に保存\n",
    "            if index % 10 == 9:\n",
    "                g.save_weights('generator', True)\n",
    "                d.save_weights('discriminator', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(BATCH_SIZE, nice=False):\n",
    "    g = generator_model()\n",
    "    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    g.load_weights('generator')\n",
    "    if nice:\n",
    "        d = discriminator_model()\n",
    "        d.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "        d.load_weights('discriminator')\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE*20, 100))\n",
    "        generated_images = g.predict(noise, verbose=1)\n",
    "        d_pret = d.predict(generated_images, verbose=1)\n",
    "        index = np.arange(0, BATCH_SIZE*20)\n",
    "        index.resize((BATCH_SIZE*20, 1))\n",
    "        pre_with_index = list(np.append(d_pret, index, axis=1))\n",
    "        pre_with_index.sort(key=lambda x: x[0], reverse=True)\n",
    "        nice_images = np.zeros((BATCH_SIZE,) + generated_images.shape[1:3], dtype=np.float32)\n",
    "        nice_images = nice_images[:, :, :, None]\n",
    "        for i in range(BATCH_SIZE):\n",
    "            idx = int(pre_with_index[i][1])\n",
    "            nice_images[i, :, :, 0] = generated_images[idx, :, :, 0]\n",
    "        image = combine_images(nice_images)\n",
    "    else:\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "        generated_images = g.predict(noise, verbose=1)\n",
    "        image = combine_images(generated_images)\n",
    "    image = image*127.5+127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\"generated_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 0\n",
      "Number of batches 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cagpie\\anaconda3\\envs\\v3.5\\lib\\site-packages\\keras\\engine\\training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 d_loss : 0.703093\n",
      "batch 0 g_loss : 0.404721\n",
      "batch 1 d_loss : 0.462444\n",
      "batch 1 g_loss : 0.442370\n",
      "batch 2 d_loss : 0.435681\n",
      "batch 2 g_loss : 0.799561\n",
      "batch 3 d_loss : 0.362454\n",
      "batch 3 g_loss : 1.609449\n",
      "batch 4 d_loss : 0.287261\n",
      "batch 4 g_loss : 2.684033\n",
      "batch 5 d_loss : 0.235744\n",
      "batch 5 g_loss : 3.574362\n",
      "batch 6 d_loss : 0.205323\n",
      "batch 6 g_loss : 4.239281\n",
      "batch 7 d_loss : 0.206319\n",
      "batch 7 g_loss : 4.551203\n",
      "batch 8 d_loss : 0.218894\n",
      "batch 8 g_loss : 5.005606\n",
      "batch 9 d_loss : 0.236090\n",
      "batch 9 g_loss : 5.646854\n",
      "batch 10 d_loss : 0.278685\n",
      "batch 10 g_loss : 5.220964\n",
      "batch 11 d_loss : 0.279124\n",
      "batch 11 g_loss : 4.333030\n",
      "batch 12 d_loss : 0.317399\n",
      "batch 12 g_loss : 3.749338\n",
      "batch 13 d_loss : 0.216050\n",
      "batch 13 g_loss : 3.509508\n",
      "batch 14 d_loss : 0.203335\n",
      "batch 14 g_loss : 3.140157\n",
      "batch 15 d_loss : 0.171105\n",
      "batch 15 g_loss : 3.497346\n",
      "batch 16 d_loss : 0.189665\n",
      "batch 16 g_loss : 3.635487\n",
      "batch 17 d_loss : 0.155715\n",
      "batch 17 g_loss : 3.963567\n",
      "batch 18 d_loss : 0.178260\n",
      "batch 18 g_loss : 4.146701\n",
      "batch 19 d_loss : 0.175201\n",
      "batch 19 g_loss : 4.492440\n",
      "batch 20 d_loss : 0.161789\n",
      "batch 20 g_loss : 4.232631\n",
      "batch 21 d_loss : 0.180712\n",
      "batch 21 g_loss : 4.222257\n",
      "batch 22 d_loss : 0.169194\n",
      "batch 22 g_loss : 4.521101\n",
      "batch 23 d_loss : 0.137969\n",
      "batch 23 g_loss : 5.212426\n",
      "batch 24 d_loss : 0.161380\n",
      "batch 24 g_loss : 4.564509\n",
      "batch 25 d_loss : 0.254571\n",
      "batch 25 g_loss : 4.777457\n",
      "batch 26 d_loss : 0.292837\n",
      "batch 26 g_loss : 4.106550\n",
      "batch 27 d_loss : 0.301957\n",
      "batch 27 g_loss : 4.064731\n",
      "batch 28 d_loss : 0.153447\n",
      "batch 28 g_loss : 4.439498\n",
      "batch 29 d_loss : 0.099491\n",
      "batch 29 g_loss : 5.045664\n",
      "batch 30 d_loss : 0.057671\n",
      "batch 30 g_loss : 5.238582\n",
      "batch 31 d_loss : 0.072430\n",
      "batch 31 g_loss : 5.283256\n",
      "batch 32 d_loss : 0.061011\n",
      "batch 32 g_loss : 5.629125\n",
      "batch 33 d_loss : 0.063813\n",
      "batch 33 g_loss : 5.410325\n",
      "batch 34 d_loss : 0.065211\n",
      "batch 34 g_loss : 5.271471\n",
      "batch 35 d_loss : 0.054820\n",
      "batch 35 g_loss : 5.391608\n",
      "batch 36 d_loss : 0.064661\n",
      "batch 36 g_loss : 5.380717\n",
      "batch 37 d_loss : 0.062707\n",
      "batch 37 g_loss : 5.327015\n",
      "batch 38 d_loss : 0.048772\n",
      "batch 38 g_loss : 5.365167\n",
      "batch 39 d_loss : 0.044750\n",
      "batch 39 g_loss : 5.592533\n",
      "batch 40 d_loss : 0.045934\n",
      "batch 40 g_loss : 6.027043\n",
      "batch 41 d_loss : 0.055412\n",
      "batch 41 g_loss : 6.028568\n",
      "batch 42 d_loss : 0.050061\n",
      "batch 42 g_loss : 5.721344\n",
      "batch 43 d_loss : 0.043248\n",
      "batch 43 g_loss : 6.244526\n",
      "batch 44 d_loss : 0.044483\n",
      "batch 44 g_loss : 5.800158\n",
      "batch 45 d_loss : 0.040759\n",
      "batch 45 g_loss : 6.023045\n",
      "batch 46 d_loss : 0.047294\n",
      "batch 46 g_loss : 5.995610\n",
      "batch 47 d_loss : 0.053645\n",
      "batch 47 g_loss : 6.008294\n",
      "batch 48 d_loss : 0.047826\n",
      "batch 48 g_loss : 6.333314\n",
      "batch 49 d_loss : 0.062400\n",
      "batch 49 g_loss : 6.311041\n",
      "batch 50 d_loss : 0.042678\n",
      "batch 50 g_loss : 6.638378\n",
      "batch 51 d_loss : 0.041184\n",
      "batch 51 g_loss : 6.312063\n",
      "batch 52 d_loss : 0.037661\n",
      "batch 52 g_loss : 6.673575\n",
      "batch 53 d_loss : 0.041917\n",
      "batch 53 g_loss : 7.074677\n",
      "batch 54 d_loss : 0.042474\n",
      "batch 54 g_loss : 7.442119\n",
      "batch 55 d_loss : 0.029446\n",
      "batch 55 g_loss : 7.629589\n",
      "batch 56 d_loss : 0.030224\n",
      "batch 56 g_loss : 7.181312\n",
      "batch 57 d_loss : 0.031495\n",
      "batch 57 g_loss : 7.093780\n",
      "batch 58 d_loss : 0.031284\n",
      "batch 58 g_loss : 7.268577\n",
      "batch 59 d_loss : 0.032313\n",
      "batch 59 g_loss : 7.270572\n",
      "batch 60 d_loss : 0.028848\n",
      "batch 60 g_loss : 7.383101\n",
      "batch 61 d_loss : 0.030219\n",
      "batch 61 g_loss : 7.741811\n",
      "batch 62 d_loss : 0.035057\n",
      "batch 62 g_loss : 7.216471\n",
      "batch 63 d_loss : 0.054747\n",
      "batch 63 g_loss : 7.516622\n",
      "batch 64 d_loss : 0.081620\n",
      "batch 64 g_loss : 7.921124\n",
      "batch 65 d_loss : 0.043019\n",
      "batch 65 g_loss : 7.852572\n",
      "batch 66 d_loss : 0.041489\n",
      "batch 66 g_loss : 8.377247\n",
      "batch 67 d_loss : 0.100387\n",
      "batch 67 g_loss : 7.312178\n",
      "batch 68 d_loss : 0.412008\n",
      "batch 68 g_loss : 8.365993\n",
      "batch 69 d_loss : 0.068295\n",
      "batch 69 g_loss : 9.080473\n",
      "batch 70 d_loss : 0.414692\n",
      "batch 70 g_loss : 5.885280\n",
      "batch 71 d_loss : 1.249629\n",
      "batch 71 g_loss : 4.853702\n",
      "batch 72 d_loss : 0.497067\n",
      "batch 72 g_loss : 6.458161\n",
      "batch 73 d_loss : 0.341565\n",
      "batch 73 g_loss : 7.005809\n",
      "batch 74 d_loss : 0.160770\n",
      "batch 74 g_loss : 8.275076\n",
      "batch 75 d_loss : 0.026610\n",
      "batch 75 g_loss : 9.020317\n",
      "batch 76 d_loss : 0.129408\n",
      "batch 76 g_loss : 7.821181\n",
      "batch 77 d_loss : 0.053687\n",
      "batch 77 g_loss : 7.364861\n",
      "batch 78 d_loss : 0.097738\n",
      "batch 78 g_loss : 7.935400\n",
      "batch 79 d_loss : 0.074582\n",
      "batch 79 g_loss : 9.099749\n",
      "batch 80 d_loss : 0.036839\n",
      "batch 80 g_loss : 8.914204\n",
      "batch 81 d_loss : 0.050312\n",
      "batch 81 g_loss : 8.487018\n",
      "batch 82 d_loss : 0.079741\n",
      "batch 82 g_loss : 8.508764\n",
      "batch 83 d_loss : 0.121297\n",
      "batch 83 g_loss : 9.258995\n",
      "batch 84 d_loss : 0.159693\n",
      "batch 84 g_loss : 7.655852\n",
      "batch 85 d_loss : 0.277013\n",
      "batch 85 g_loss : 8.431674\n",
      "batch 86 d_loss : 0.098637\n",
      "batch 86 g_loss : 8.368351\n",
      "batch 87 d_loss : 0.145790\n",
      "batch 87 g_loss : 7.314304\n",
      "batch 88 d_loss : 0.353083\n",
      "batch 88 g_loss : 8.244926\n",
      "batch 89 d_loss : 0.075665\n",
      "batch 89 g_loss : 7.808215\n",
      "batch 90 d_loss : 0.050240\n",
      "batch 90 g_loss : 6.951924\n",
      "batch 91 d_loss : 0.048179\n",
      "batch 91 g_loss : 7.026190\n",
      "batch 92 d_loss : 0.047057\n",
      "batch 92 g_loss : 7.004433\n",
      "batch 93 d_loss : 0.036898\n",
      "batch 93 g_loss : 6.687402\n",
      "batch 94 d_loss : 0.036436\n",
      "batch 94 g_loss : 6.106062\n",
      "batch 95 d_loss : 0.030684\n",
      "batch 95 g_loss : 6.057693\n",
      "batch 96 d_loss : 0.035150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-54620cbf7d72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-5879fc23ab9c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(BATCH_SIZE)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;31m# Gを更新\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdcgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"batch %d g_loss : %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;31m# 重みを定期的に保存\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\cagpie\\anaconda3\\envs\\v3.5\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m   1046\u001b[0m         return self.model.train_on_batch(x, y,\n\u001b[0;32m   1047\u001b[0m                                          \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m                                          class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m     def test_on_batch(self, x, y,\n",
      "\u001b[1;32mc:\\users\\cagpie\\anaconda3\\envs\\v3.5\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1837\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1839\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1840\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\cagpie\\anaconda3\\envs\\v3.5\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\cagpie\\anaconda3\\envs\\v3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\cagpie\\anaconda3\\envs\\v3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\cagpie\\anaconda3\\envs\\v3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mc:\\users\\cagpie\\anaconda3\\envs\\v3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\cagpie\\anaconda3\\envs\\v3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(BATCH_SIZE=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 157us/step\n",
      "2000/2000 [==============================] - 0s 124us/step\n"
     ]
    }
   ],
   "source": [
    "generate(BATCH_SIZE=100, nice=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v3.5",
   "language": "python",
   "name": "v3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
